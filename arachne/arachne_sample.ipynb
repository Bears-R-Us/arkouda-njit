{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1992821c",
   "metadata": {},
   "source": [
    "### **Introduction to Arachne**\n",
    "Arachne is a Python package for graph analysis that is built as an extension to Arkouda, a Python package for analysis on tabular data, akin to NumPy and Pandas. In this notebook we will show examples on how to run each algorithm that has been implemented on different types of graphs: undirected, directed, and property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arkouda as ak\n",
    "import arachne as ar\n",
    "import scipy as sp\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Make sure to change the server name to whatever is applicable in your environment. If running locally, then use only ak.connect().\n",
    "ak.connect(\"n32\", 5555)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2d6b6e",
   "metadata": {},
   "source": [
    "### **Graph Generation and Loading**\n",
    "Graphs can be built from existing data or generated with our suite of random graph generators. The preferred way to load a graph into memory is from Arkouda arrays, however we provide a method to read a graph in from matrix market format or randomly generate some graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b6a0f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in a graph from a matrix market file.\n",
    "absolute_path_to_karate = os.path.abspath(\"data/karate.mtx\")\n",
    "karate = ar.read_matrix_market_file(absolute_path_to_karate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b625a57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a random graph using any variety of random generators available.\n",
    "n = 100\n",
    "rmat_graph = ar.rmat(15, create_using=ar.Graph)\n",
    "gnp_graph = ar.gnp(n, 0.75, create_using=ar.Graph)\n",
    "rtree_graph = ar.random_tree(n, create_using=ar.Graph)\n",
    "ws_graph = ar.watts_strogatz_graph(n, 25, 0.56, create_using=ar.Graph)\n",
    "graph_list = [rmat_graph, gnp_graph, rtree_graph, ws_graph]\n",
    "for g in graph_list:\n",
    "    print(f\"Generated graph has {len(g)} vertices and {g.size()} edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47d13bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a property graph from Arkouda dataframes, usually read in from HDF5, Parquet, or CSV files. For demonstrative purposes, we create some random dataset here.\n",
    "n = 1_000           # Number of vertices.\n",
    "m = 1_000_000       # Number of edges.\n",
    "k = 2               # Value to cap the randomness at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d0712b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create variously different random arrays of different types: integers, unsigned integers, floats, booleans, strings, and categoricals.\n",
    "src_array = ak.randint(0, n, m, dtype=ak.dtype('int64'), seed=2)\n",
    "dst_array = ak.randint(0, n, m, dtype=ak.dtype('int64'), seed=4)\n",
    "int_array = ak.randint(-1, k, m, dtype=ak.dtype('int64'), seed=6)\n",
    "uint_array = ak.randint(0, k, m, dtype=ak.dtype('uint64'), seed=8)\n",
    "real_array = ak.randint(0, k, m, dtype=ak.dtype('float64'), seed=10)\n",
    "bool_array = ak.randint(0, k, m, dtype=ak.dtype('bool'), seed=12)\n",
    "strings_array = ak.random_strings_uniform(0, k, m, characters=\"abcdefghijklmonpqrstuvwxyz\", seed=14)\n",
    "categorical_array = ak.Categorical(ak.random_strings_uniform(0, k, m, characters=\"abcdefghijklmonpqrstuvwxyz\", seed=14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c9b38b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty graph object.\n",
    "prop_graph = ar.PropGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af0f2c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with the edge data.\n",
    "test_edge_dict = {\n",
    "    \"src\":src_array,\n",
    "    \"dst\":dst_array,\n",
    "    \"data1\":int_array,\n",
    "    \"data2\":uint_array,\n",
    "    \"data3\":real_array,\n",
    "    \"data4\":bool_array,\n",
    "    \"data5\":strings_array,\n",
    "    \"data6\":categorical_array\n",
    "}\n",
    "test_edge_df = ak.DataFrame(test_edge_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adda3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_edge_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca0bbe6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the edge attributes, with sorts the edges and handles storing their data.\n",
    "prop_graph.load_edge_attributes(test_edge_df, source_column=\"src\", destination_column=\"dst\", relationship_columns=[\"data5\", \"data1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e94bf24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sizes for vertex information.\n",
    "m = len(prop_graph)\n",
    "k = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5dc37703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data of different types for vertices.\n",
    "int_array = ak.randint(-1, k, m, dtype=ak.dtype('int64'), seed=6)\n",
    "uint_array = ak.randint(0, k, m, dtype=ak.dtype('uint64'), seed=8)\n",
    "real_array = ak.randint(0, k, m, dtype=ak.dtype('float64'), seed=10)\n",
    "bool_array = ak.randint(0, k, m, dtype=ak.dtype('bool'), seed=12)\n",
    "strings_array = ak.random_strings_uniform(0, k, m, characters=\"abcdefghijklmonpqrstuvwxyz\", seed=14)\n",
    "categorical_array = ak.Categorical(ak.random_strings_uniform(0, k, m, characters=\"abcdefghijklmonpqrstuvwxyz\", seed=14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d849d4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with vertex data.\n",
    "test_node_dict = {\n",
    "    \"nodes\":prop_graph.nodes(),\n",
    "    \"data1\":int_array,\n",
    "    \"data2\":uint_array,\n",
    "    \"data3\":real_array,\n",
    "    \"data4\":bool_array,\n",
    "    \"data5\":strings_array,\n",
    "    \"data6\":categorical_array\n",
    "}\n",
    "test_node_df = ak.DataFrame(test_node_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c54e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_node_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56581839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the vertex data.\n",
    "prop_graph.load_node_attributes(test_node_df, node_column=\"nodes\", label_columns=[\"data5\", \"data2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f19115",
   "metadata": {},
   "source": [
    "### **Graph Processing and Querying**\n",
    "Treating the graphs as dataframes allows us to exploit Arkouda's array searches to generate subgraphs in seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2343803e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create filters for vertices.\n",
    "def node_filter(node_attributes):\n",
    "    return node_attributes[\"data2\"] == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94b1b23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create filters for edges.\n",
    "def edge_filter(edge_attributes):\n",
    "    return edge_attributes[\"data1\"] > -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce31b434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create different subgraphs from each demo and together.\n",
    "subgraph_nodes = prop_graph.subgraph_view(filter_node=node_filter)\n",
    "print(f\"Subgraph generated with edge size: {subgraph_nodes.size()}\")\n",
    "subgraph_edges = prop_graph.subgraph_view(filter_edge=edge_filter)\n",
    "print(f\"Subgraph generated with edge size: {subgraph_edges.size()}\")\n",
    "subgraph_together = prop_graph.subgraph_view(filter_node=node_filter, filter_edge=edge_filter)\n",
    "print(f\"Subgraph generated with edge size: {subgraph_together.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eded8d6c",
   "metadata": {},
   "source": [
    "### **Graph Algorithms**\n",
    "Let's now revisit the karate graph from above and do some analyses with Arachne and NetworkX together! First, let's start out by reading the matrix market file again, both with Arachne and NetworkX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "61aa774c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's build the graph with Arachne.\n",
    "filepath = os.path.abspath(\"data/karate.mtx\")\n",
    "G = ar.read_matrix_market_file(filepath)\n",
    "\n",
    "edge_src, edge_dst = G.edges()\n",
    "edge_src = edge_src.to_list()\n",
    "edge_dst = edge_dst.to_list()\n",
    "edge_list = []\n",
    "for (u,v) in zip(edge_src,edge_dst):\n",
    "    edge_list.append((u,v))\n",
    "nodes = G.nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7378b0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Secondly, let's build the graph with NetworkX. NOTE: Arachne has a direct read_matrix_market_file method whereas NetworkX requires you to use SciPy to read in the matrix market file.\n",
    "fh = open(filepath, \"rb\")\n",
    "H = nx.from_scipy_sparse_array(sp.io.mmread(fh))\n",
    "print(f\"Let's make sure that both graphs have the same number of vertices and edges. Arachne graph has {len(G)} vertices and {G.size()} edges. NetworkX has {len(H)} vertices and {H.size()} edges.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dadb6a",
   "metadata": {},
   "source": [
    "We are also able to display our graph with NetworkX methods by exporting our edgelist and building a graph from the edges stored by Arachne. **Note: Visualization is resource-intensive, so you can really only do it for small graphs. However, we perform these steps to show how Arachne can complement NetworkX and how graphs can be exchanged between them.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ef43f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx_display = nx.Graph()\n",
    "nx_display.add_edges_from(edge_list)\n",
    "\n",
    "pos = nx.kamada_kawai_layout(nx_display)\n",
    "nx.draw_networkx(nx_display, pos, with_labels=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cba34d",
   "metadata": {},
   "source": [
    "Below we also see how we can extract a degree view for a graph to see the dispersion of degrees amongst the vertices. This also works for directed graphs by using the `G.in_degree()` and `G.out_degree()` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07067f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = G.degree()\n",
    "print(degrees)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f7772e",
   "metadata": {},
   "source": [
    "Using Arkouda arrays we can also extract the node with maximum degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33d3fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Node {G.nodes()[ak.argmax(degrees)]} has maximum degree of {ak.max(degrees)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b71fed3",
   "metadata": {},
   "source": [
    "### **Breadth-First Search**\n",
    "\n",
    "Given a graph $G$ (as defined above) and a source vertex $s$, breadth-first search (BFS) traverses the graph in a level-centric manner. It can return the tree inherently generated during BFS, the vertices found at each layer, or an ordering of nodes as predecessors or successors. For our implementation we opted to mimic NetworkX's `bfs_layers` function that returns the nodes and the layer they belong to.\n",
    "\n",
    "Our BFS method, written in Chapel, expands the next frontier in parallel on each locale, if it is run on a distributed cluster. Each locale iterates over the frontier, and if there are any local edges, it inspects the neighbors, and if not yet visited, updates the `depth` (layer) pdarray. The final `depth` array is returned which can be used to provide a different view of a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d4c13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = ar.bfs_layers(G, 1)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1345792d",
   "metadata": {},
   "source": [
    "We may also extract a histogram of the sizes of each level computed by BFS. This can be a useful manner in seeing how how big each group of vertices at a particular level is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bef7b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_histogram = ak.histogram(d, bins=ak.max(d)+1)\n",
    "print(d_histogram[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8924b9",
   "metadata": {},
   "source": [
    "From the above we can see that at level 0 we obviously only have one vertex. Then, we can see that at level 1 we have 16, level 2 we have 9, and level 3 we have 8. The maximum level is 3. What happens if we change the source vertex? Let's see the results of bfs_layers for each vertex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bc98b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,len(G)+1):\n",
    "    d = ar.bfs_layers(G, i)\n",
    "    d_histogram = ak.histogram(d, bins=ak.max(d)+1)\n",
    "    print(f\"Size of breadth-first search layers for vertex {i}: {d_histogram[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13d9f2a",
   "metadata": {},
   "source": [
    "From the output above we notice some main things:\n",
    "1. Starting BFS from different sources arise to different number of layers for the output of each vertex.\n",
    "2. Some nodes have the same number of vertices at each layer. \n",
    "3. Vertex 19 provides the largest number of layers for any of the vertices. \n",
    "\n",
    "Let's color the nodes for node 19 and show the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c100625a",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = ar.bfs_layers(G, 19)\n",
    "color_dict = {0:\"blue\", 1:\"green\", 2:\"red\", 3:\"purple\", 4:\"orange\", 5:\"yellow\"}\n",
    "print(d)\n",
    "\n",
    "color_map = []\n",
    "nodes = nx_display.nodes()\n",
    "for u in nodes:\n",
    "    color_map.append(color_dict[d[u-1]])\n",
    "    \n",
    "print(nx_display.nodes)\n",
    "\n",
    "pos = nx.kamada_kawai_layout(nx_display)\n",
    "nx.draw_networkx(nx_display, pos, with_labels=True, node_color=color_map)\n",
    "plt.show()\n",
    "\n",
    "print(f\"degrees = {degrees}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891c38f1",
   "metadata": {},
   "source": [
    "In the returned image we can see the first frontier are the green vertices $<0, 1, 33>$. If we look at the degree view of the graph we notice that 0, 1, and 33 are in the top 4 of vertices with most degree, which now makes sense why vertex 19 had the largest expanded frontier. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056f8568",
   "metadata": {},
   "source": [
    "### **Connected Components**\n",
    "\n",
    "Given a graph G, (as defined above) if there is a path from a vertex $u$ to every other vertex $v$ then the graph is said to be connected. If there is not a path, then the graph is said to be disconnected and composed of multiple connected components. There may be a large number of connected components of varying sizes in a graph.\n",
    "\n",
    "One of the most ancient manners of calculating connected components involve running BFS until every vertex has been visited, at every iteration looking for the node whose value `-1` and start BFS from it. These steps are repeated until all the vertices have been labeled. The other is using union-find to build a tree induced by each connected component. For our method we use a lbel propagation technique that sends the minimum vertex label to all the other vertices in a connected component.\n",
    "\n",
    "To run our connected components, you just have to call the `connected_components()` method. **We use one of the randomly generated graphs above for this example**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab22ea5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = ar.connected_components(rmat_graph)\n",
    "print(cc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9dfc0a",
   "metadata": {},
   "source": [
    "We can use Arkouda methods to get the size of each component in our file. This will let us know which vertices will be included in the induced subgraph of the largest component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3affcb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = ak.GroupBy(cc)\n",
    "keys, count = g.size()\n",
    "label_of_largest_component = ak.argmax(count)\n",
    "label_of_smallest_component = ak.argmin(count)\n",
    "print(f\"The largest component is labeled: {keys[label_of_largest_component]}\")\n",
    "print(f\"The smallest component is labeled: {keys[label_of_smallest_component]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26d4aab",
   "metadata": {},
   "source": [
    "### **Triangle Counting**\n",
    "One of the most basic community structures that can be found in graphs are triangles. Triangles are 3-cliques in an undirected graph which means three nodes are strongly connected to each other. Triangle counting can be used to detect how cohesive communities are, the more triangles there are, the better connected a community is. It can also be used to drive other graph analytical algorithms such as centrality measures (triangle centrality) and substructure detection (k-truss analytics).\n",
    "\n",
    "Our triangle counting method involves inspecting every edge in a graph and the adjacency lists of both endpoints to find the intersection point. Most algorithms perform list intersection, for our case we perform a binary search of every vertex in the smaller adjacency list into the bigger one. This allows us to efficiently find triangles with less work.\n",
    "\n",
    "Please note that you have to divide the total number of triangles found by 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef580d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_tri = ar.triangles(G)\n",
    "print(f\"The whole graph has {G_tri/3} triangles.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3119c65",
   "metadata": {},
   "source": [
    "You can also pass an array of vertex names to return only the number of triangles those vertices belong to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922ec7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_tris = ar.triangles(G, ak.array([1,19]))\n",
    "print(f\"Vertex 1 has {G_tris[0]} triangles and vertex 19 has {G_tris[1]} triangles.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7808a40",
   "metadata": {},
   "source": [
    "### **Truss Analytics**\n",
    "\n",
    "Given a graph $G$ (as defined above), the $k$-truss of a graph is one where every edge is a part of least $k-2$ triangles. This creates a cohesive subgraph where only the edges that meet these requirement are kept. It involves recalculating the number of triangles multiple times, which we avoid by tracking the support of each edge at every iteration of the algorithm. We provide novel algorithmic implementations for three truss analytical algorithms:\n",
    "1. $k$-truss\n",
    "2. max-truss\n",
    "3. truss decomposition\n",
    "\n",
    "Below we run `k_truss` and display the edges that belong to at least 2 triangles. We could build an induced subgraph from these edges in the same manner as we showed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453d1389",
   "metadata": {},
   "outputs": [],
   "source": [
    "kt = ar.k_truss(G, 4)\n",
    "print(kt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f569d48",
   "metadata": {},
   "source": [
    "Here, we see that the maximum truss of this graph is 5. This means that the $k$-truss is no longer defined for any value of $k$ larger than 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cf789b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mt = ar.max_truss(G)\n",
    "print(mt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebe66d5",
   "metadata": {},
   "source": [
    "Lastly, the truss decomposition shows the trussness of every edge. This means that the largest value of k for every edge is the one presented in the edge index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9453589",
   "metadata": {},
   "outputs": [],
   "source": [
    "td = ar.truss_decomposition(G)\n",
    "print(td)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada7b537",
   "metadata": {},
   "source": [
    "Using similar coloring steps as above, we can actually color the edges of the graph. The steps for that are below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bfe6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_dict = {5:\"red\", 4:\"purple\", 3:\"green\", 2:\"blue\"}\n",
    "edge_color = []\n",
    "\n",
    "for i in range(G.size()):\n",
    "    edge_color.append(color_dict[td[i]])\n",
    "    \n",
    "pos = nx.spring_layout(nx_display, seed=200)\n",
    "nx.draw_networkx(nx_display, pos, with_labels=True, edge_color=edge_color)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f764afd",
   "metadata": {},
   "source": [
    "### **Triangle Centrality**\n",
    "\n",
    "Triangle centrality is a centrality measure to decide how important a vertex is based off how many triangles surround a particular vertex. It is calculated based off the following formula as seen in the paper titled \"Triangle Centrality\" by Paul Burkhardt.\n",
    "\n",
    "$$TC(v) = \\frac{\\frac{1}{3} \\sum_{u \\in N_{\\Delta}^{+}(v)}{\\Delta(u)} + \\sum_{w \\in (N(v) \\backslash N_{\\Delta}(v))} {\\Delta (w)}}{\\Delta(G)}$$\n",
    "\n",
    "Where $N(v)$ is the neighborhood set of a vertex $v$, $N_{\\Delta}(v)$ is the set of neighbors that are in triangles with $v$, and $N_{\\Delta}^{+}$ is the closed set that includes $v$. There is not an equivalent metric of this form available in NetworkX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa9be7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tce = ar.triangle_centrality(G)\n",
    "print(f\"One of the most important vertices in the graph is: {G.nodes()[ak.argmax(tce)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f3f8ea",
   "metadata": {},
   "source": [
    "### **Subgraph Isomorphism**\n",
    "\n",
    "The problem of motif finding in graphs is one of pattern matching with a smaller subgraph to search inside of a larger host graph. Here, we have an implementation of parallel VF2 that returns the subgraph mappings of vertices inside of the graph that contain the same structure as the given subgraph. Currently, our subgraph isomorphism method works only for property graphs. Further, it returns monomorphisms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7fd4d820",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_directed = ar.PropGraph()\n",
    "edges_to_add = G.edges()\n",
    "G_directed.add_edges_from(edges_to_add[0], edges_to_add[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "999dfc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "subgraph = ar.PropGraph()\n",
    "src = [0, 1, 2, 1]\n",
    "dst = [1, 2, 0, 3]\n",
    "subgraph.add_edges_from(ak.array(src), ak.array(dst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "36d90c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "isos = ar.subgraph_isomorphism(G_directed, subgraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db263405",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"We found {len(isos)/4} triangles with tails inside of the karate graph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a27650",
   "metadata": {},
   "source": [
    "### **Diameter**\n",
    "\n",
    "The diameter of a graph is the longest of the shortest paths between two vertices in a graph. Here, we should an experimental version that approximates the diameter with connected components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bb8a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "di = ar.diameter(G)\n",
    "print(f\"The diameter of karate is {di}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7ff40d",
   "metadata": {},
   "source": [
    "### **Well-Connected Components**\n",
    "\n",
    "Performs steps to confirm if a component within a community is well-connected or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21750aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cluster_dict = {}\n",
    "\n",
    "# Read the file and populate the dictionary\n",
    "with open('/scratch/users/md724/DataSets/wcc/test_clustering.tsv', 'r') as file:\n",
    "    for line in file:\n",
    "        # Remove any leading/trailing whitespace\n",
    "        line = line.strip()\n",
    "        # Skip empty lines\n",
    "        if not line:\n",
    "            continue\n",
    "        # Split the line into node and cluster number\n",
    "        node_str, cluster_str = line.split('\\t')\n",
    "        node = int(node_str)\n",
    "        cluster_num = int(cluster_str)\n",
    "        # Add the node to the corresponding cluster\n",
    "        if cluster_num in cluster_dict:\n",
    "            cluster_dict[cluster_num].append(node)\n",
    "        else:\n",
    "            cluster_dict[cluster_num] = [node]\n",
    "\n",
    "# Find the cluster with the maximum number of nodes\n",
    "max_cluster_num = None\n",
    "max_cluster_nodes = []\n",
    "max_cluster_size = 0\n",
    "\n",
    "for cluster_num, nodes in cluster_dict.items():\n",
    "    cluster_size = len(nodes)\n",
    "    if cluster_size > max_cluster_size:\n",
    "        max_cluster_size = cluster_size\n",
    "        max_cluster_num = cluster_num\n",
    "        max_cluster_nodes = nodes\n",
    "\n",
    "# Assign the nodes of the biggest cluster to the variable 'cluster'\n",
    "cluster = max_cluster_nodes\n",
    "\n",
    "# Print the cluster and its size\n",
    "#print(f\"cluster = {cluster};\")\n",
    "print(f\"Cluster {max_cluster_num} is the biggest cluster with {max_cluster_size} nodes\")\n",
    "\n",
    "\n",
    "# Read the TSV file using pandas\n",
    "network_df = pd.read_csv(\"/scratch/users/md724/DataSets/wcc/test_network.tsv\", sep=\"\\t\", header=None, names=[\"src\", \"dst\"])\n",
    "network_df['type'] = 'T1'\n",
    "\n",
    "# Print the pandas DataFrame (optional, just to verify)\n",
    "print(network_df)\n",
    "print(network_df.head())\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "# Concatenate 'src' and 'dst' columns and find unique elements for nodes\n",
    "unique_nodes = pd.concat([network_df['src'], network_df['dst']]).unique()\n",
    "\n",
    "# Number of unique nodes\n",
    "num_unique_nodes = len(unique_nodes)\n",
    "#num_unique_nodes\n",
    "\n",
    "# %%\n",
    "# Number of edges is the length of the DataFrame\n",
    "num_edges = len(network_df)\n",
    "\n",
    "# Calculate graph density\n",
    "d = num_edges / (num_unique_nodes * (num_unique_nodes - 1))\n",
    "print(f\"Graph Density: {d}\")\n",
    "\n",
    "print(f\"Number of unique nodes: {num_unique_nodes}\")\n",
    "print(f\"Number of edges: {num_edges}\")\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "# Convert the pandas DataFrame to an Arkouda DataFrame\n",
    "ak_network_df = ak.DataFrame(network_df)\n",
    "ak_network_df\n",
    "\n",
    "# %%\n",
    "# Create an Arachne PropGraph and load the edge attributes\n",
    "ar_network_graph = ar.PropGraph()\n",
    "ar_network_graph.load_edge_attributes(ak_network_df, source_column=\"src\", destination_column=\"dst\", relationship_columns=[\"type\"])\n",
    "\n",
    "\n",
    "# %%\n",
    "# Get all nodes and create node attributes\n",
    "all_nodes = ak.concatenate([ak_network_df['src'], ak_network_df['dst']])\n",
    "unique_nodes = ak.unique(all_nodes)\n",
    "lbls = ak.array([\"1\"] * unique_nodes.size)\n",
    "network_node_df = ak.DataFrame({\"nodes\": unique_nodes, \"lbls\": lbls})\n",
    "network_node_df\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "ar_network_graph.load_node_attributes(network_node_df, node_column=\"nodes\", label_columns=[\"lbls\"])\n",
    "ar_network_graph\n",
    "\n",
    "# %%\n",
    "print(\"Running Arachne\")\n",
    "filePath = \"/scratch/users/md724/DataSets/wcc/clustering.tsv\"\n",
    "print(type(ar_network_graph))  # Check if it's PropGraph\n",
    "print(type(filePath)) \n",
    "clusters = ar.well_connected_components(ar_network_graph,filePath)\n",
    "\n",
    "print(\"clusters = \", clusters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arkouda-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
